---
layout: post
title: Storm(1)
categories: [general, setup, demo]
tags: [demo, dbyll, dbtek, setup]
fullview: false
comments: true
---


# Storm의 컴포넌트

### 토폴로지(Topology)
토폴로지에 정의된 각 객체는 하나의 처리 로직을 포함한다. 토폴로지는 데이터를 읽어올 스트림을 정의할 수 있고, 읽어 들인 스트림을 처리할 처리 로직을 포함할 수 있다.

### 스트림(Stream)
Storm에서는 일련의 튜플(tuple)의 흐름을 스트림으로 정의하고 있다. 그리고 이 스트림을 분산 환경에서 신뢰성 있게 다른 스트림으로 전환할 수 있는 기능을 제공한다. 튜플은 기본 데이터 타입(primitive data)나 바이트 배열(byte array)을 포함할 수 있고 사용자 정의 타입을 정의할 수도 있다.

### 스파우트(Spout)
스파우트는 스트림의 소스를 지칭하는 컴포넌트이다. 일반적으로 스파우트는 외부 소스로부터 튜플을 읽고 토폴로지로 튜플을 생성해 스트림을 생성하는 역할을 한다. 스트림을 처리하는 도중 실패했을때 결과를 재전송하는 신뢰성을 제공할 수 있다.

### 볼트
토폴로지의 모든 처리 작업(필터링, aggregation, 데이터 조인, 데이터 DB 적재 등)을 볼트에서 진행한다. 볼트는 스트림을 필요한 데이터 스트림으로 변환한다. 필요한 데이터 스트림을 얻으려면 처리 작업을 기능적으로 분리해 여러 개의 볼트를 정의하고 각 볼트를 수행하는 단계가 필요할 수도 있다. 예를 들어 트윗 스트림을 인기 있는 이미지의 스트림으로 변환하려면 적어도 두 종류의 단계가 필요하다. 리트윗된 이미지의 횟수를 계산하는 볼트와 그 중에서 top-N에 해당하는 이미지를 추출하는 볼트가 필요하기 때문이다.

<br>

# 데이타 스트림 처리를 이루는 기술들

### 스트리밍 처리
먼저 데이터 스트림을 처리하기 위한 스트리밍 시스템이 필요하다. 여기서 스트리밍 시스템이란 데이터 스트림을 여러가지 경로로 분산하고 각각의 단계별로 처리(사용자 구매 패턴 분석, 구매 패턴 검증 등)하는 Work flow 기능이 필요하다.

### 대용량 분산 큐
다음으로 대용량으로 여러 경로를 통해서 들어오는 데이터를 수집하기 위한 큐가 필요하다. 비동기 처리를 위한 큐 중에서 많은 데이터를 동시에 처리하기 위한 대용량 지원성을 제공하는 Kafka와 같은 대용량 분산 큐 솔루션이 적절하다.

### 머신러닝
사용자의 거래 패턴 분석을 분석하여 이상 거래인지 검출하려면 사용자의 거래 패턴을 알려줄 필요가 있는데 이는 기존의 사용자 거래 내역을 학습하여 패턴을 분석해내는 머신러닝 기술이 필요하며, Apache Mahout, Microsoft Azure ML, Spark ML 등이 있다.
