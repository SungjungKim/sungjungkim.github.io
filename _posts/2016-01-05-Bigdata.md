---
layout: post
title: Storm vs. Spark
categories: [general, bigdata]
tags: [bigdata, storm, spark]
fullview: false
comments: true
---


# Storm

실시간 스트리밍을 처리하기 위한 서버이자 프레임워크이다.

### Spout
Spout은 외부의 로그 파일이나 데이터 스트림, 큐 등으로부터 데이터를 읽는 Storm 클러스터이다. 읽은 데이터를 다른 Bolt로 전달하는 역할을한다.

* **open()** : Spout이 초기화될 때 한 번 호출되는 메서드로서, 데이터 소스와의 연결을 초기화하는 등의 역할을 한다.
* **nextTuple()** : 데이터 스트림 하나를 읽고 나서 다음 데이터 스트림을 읽을 때 호출되는 메서드이다.
* **ask(Object msgId)** : 데이터 스트림이 성공적으로 처리되었을때 호출된다. 성공 처리된 메시지를 지우는 등, 성공에 대한 후처리를 구현한다.
* **fail(Object msgId)** : 에러나 타임아웃이 발생했을 때 호출된다. 에러에 대한 재처리나 에러 로깅 등의 처리를 한다.

### Bolt
Bolt는 읽은 데이터를 처리하는 함수이다. 데이터 스트림을 입력받고, 내부 로직에 따라 데이터를 가공한 다음 데이터 스트림으로 다른 Bolt에 넘겨주거나 종료한다.

* **prepare(Map stormConf, TopologyContext context, OutputCollector collector)** : Bolt 객체가 생성될 때 한 번 호출된다. 컨텍스트 등 초기 설정에 필요한 부분을 셋팅한다.
* **execute(Tuple input)** : Bolt에 들어온 메시지를 처리하는 로직을 담당하는 함수이다. 종단 Bolt가 아닐 경우 다음 Bolt로 메시지를 전달하기도 한다.
* **open()** : Spout이 초기화될 때 한 번 호출되는 메서드로서, 데이터 소스와의 연결을 초기화하는 등의 역할을 한다.
* **nextTuple()** : 데이터 스트림 하나를 읽고 나서 다음 데이터 스트림을 읽을 때 호출되는 메서드이다.
* **ask(Object msgId)** : 데이터 스트림이 성공적으로 처리되었을때 호출된다. 성공 처리된 메시지를 지우는 등, 성공에 대한 후처리를 구현한다.
* **fail(Object msgId)** : 에러나 타임아웃이 발생했을 때 호출된다. 에러에 대한 재처리나 에러 로깅 등의 처리를 한다.

### Bolt
Bolt는 읽은 데이터를 처리하는 함수이다. 데이터 스트림을 입력받고, 내부 로직에 따라 데이터를 가공한 다음 데이터 스트림으로 다른 Bolt에 넘겨주거나 종료한다.

* **prepare(Map stormConf, TopologyContext context, OutputCollector collector)** : Bolt 객체가 생성될 때 한 번 호출된다. 컨텍스트 등 초기 설정에 필요한 부분을 셋팅한다.
* **execute(Tuple input)** : Bolt에 들어온 메시지를 처리하는 로직을 담당하는 함수이다. 종단 Bolt가 아닐 경우 다음 Bolt로 메시지를 전달하기도 한다.

![Storm1](/img/2016/01/05/storm1.png "Storm1")

### Topology
여러 개의 Spout과 Bolt간의 연관 관계를 정의해서 데이터 흐름을 정의하는 것을 '**토폴로지(Topology)**'라고 한다.

![Storm2](/img/2016/01/05/storm2.png "Storm2")

### 장점
* Fault tolerant 구조를 통해 특정 노드에 장애가 발생하더라도 문제 없이 작업을 진행할 수 있다.
* 문제가 발생할 경우 메시지의 유실 없이 최소한 한 번 메시지가 처리될 수 있게 메시지 처리에 대한 안정성을 제공한다.
* 기본적으로 JVM(Java Virtual Machine) 위에서 동작하지만, Ruby, Python, Javascript, Perl 등 다양한 언어를 사용할 수 있다.

<br>

# Spark

### General purpose high performance distributed platform
Spark의 정의 그대로 '**범용 분산 플랫폼**'이다. 하둡과 같이 Map & Reduce 만 돌리는 것도 아니고 Storm과 같이 스트리밍 처리만 하는게 아니라, 분산된 여러 노드에서 연산을 처리할 수 있는 범용 분산 클러스터링 플랫폼이다.

Spark는 기존의 하둡이 MR(Map & Reduce) 작업을 디스크 기반으로 수행해서 느려지는 성능을 메모리 기반으로 옮겨서 가속화하기 때문에 '**메모리 하둡**'이라고 불린다.

### 주요 기능
* Map & Reduce (cf. Hadoop)
* 스트리밍 데이터 핸들링 (cf. Apache Storm)
* SQL 기반의 데이터 쿼리 (cf. Hadoop의 Hive)
* 머신 러닝 라이브러리 (cf. Apache Mahout)

### 장점
* 하나의 플랫폼 안에 배치, 스트리밍, 머신 러닝 등 다양한 처리를 제공하기 때문에 하나의 데이터로 여러가지 형태로 처리할 수 있다.
* Python, Java, Scala 등 다양한 언어를 지원하기 위한 SDK를 가지고 있다.
* Hadoop, Amazon S3, Cassandra 등 다양한 데이터 스토리지를 지원한다.
