---
layout: entry
title: Deep Learning for Beginners - 다층 신경망의 학습
author: 김성중
author-email: ajax0615@gmail.com
description: Deep Learning of Beginners의 다층 신경망의 학습 챕터에 대한 정리입니다.
publish: false
---

#### 1. 개요
단층 신경망의 근본적인 한계 때문에 신경망은 필연적으로 다층 구조로 발전할 수 밖에 없었습니다. 하지만 은닉층 하나를 추구하는데 수십 년씩이나 걸렸습니다. 문제는 다층 신경망의 학습 규칙을 찾아내지 못한 데 있었습니다.

앞서 소개한 델타 규칙으로는 다층 신경망을 학습시킬 수 없습니다. 델타 규칙으로 신경망을 학습시키려면 오차가 필요한데, 은닉층의 오차는 아예 정의조차 되어 있지 않기 때문입니다. 출력 노드의 오차는 학습 데이터의 정답과 신경망 출력값의 차이로 정의합니다. 문제는 **은닉층** 입니다. 은닉층의 노드에는 학습 데이터로부터 주어진 정답이 없습니다. 따라서 은닉층의 오차는 출력 노드와 같은 방식으로는 계산할 수가 없습니다.

1986년에 이러한 문제점은 역전파 *backpropagation* 알고리즘이 개발되면서 마침내 다층 신경망의 학습 문제가 해결되었습니다. 역전파 알고리즘의 의의는 **은닉층 노드들의 오차를 결정한 다음에는, 델타 규칙에 따라 이 오차들로 가중치들을 조절** 한다는 게 역전파 알고리즘의 핵심 아이디어입니다.

![backpropagation](/images/2017/04/13/backpropagation.png "backpropagation"){: .center-image }

신경망에서 입력 데이터는 입력층에서 은닉층을 거쳐 출력층으로 흘러갑니다. 반면 역전파 알고리즘에서는 위 그림과 같이 **신경망의 출력 오차를 출력층에서 시작해 입력층 바로 앞 은닉층까지 역순으로 이동** 시킵니다. 이 과정이 마치 출력 오차가 방향으로 퍼져나가는 것과 비슷하다고 해서 역전파 알고리즘이라는 이름이 붙었습니다. 역전파 과정에서도 신호들은 노드 간의 연결선을 따라 흐르고, 해당 연결 가중치를 곱해준다는 점 등은 똑같습니다. 각 노드에서 신호의 입출력 방향이 반대라는 점만 다릅니다.

---

#### 2. 역전파 알고리즘
은닉 노드들의 문제는 '해당 노드의 오차를 어떻게 정의하느냐'입니다. 역전파 알고리즘에서는 바로 아래(다음)에 있는 계층의 델타를 역전파시켜 얻은 가중합으로 오차를 정의합니다. **은닉 노드의 오차는 델타를 역전파시켜 얻은 가중합으로 구하고, 이 값에 활성함수의 도함수 값을 곱해 해당 노드의 델타를 구합니다.** 이러한 과정을 출력층에서 시작해 모든 은닉층에 차례대로 반복하는 게 역전파 알고리즘의 핵심입니다.

역전파 알고리즘으로 다층 신경망을 학습시키는 과정을 정리해보겠습니다.

1. 신경망의 가중치를 적당한 값으로 초기화한다.
2. 학습 데이터 { 입력, 정답 }에서 '입력'을 신경망에 입력해 출력값을 얻는다. 이 출력값과 해당 입력의 '정답'을 비교해 오차를 구하고, 출력 노드들의 델타를 계산한다.
3. 출려 노드의 델타를 역전파시켜 바로 앞(이전) 은닉 노드들의 델타를 계산한다.
4. 단계 3을 입력층 바로 앞 은닉층까지 차례로 반복한다.
5. 신경망의 가중치를 다음의 학습 규칙으로 변경한다.
6. 모든 학습 데이터에 관해 단계 2~5를 반복한다.
7. 신경망이 충분히 학습될 때까지 단계 2~6을 반복한다.

출력층의 델타를 역전파시켜 은닉층 노드들의 델타를 구하는 단계 3과 단계4가 추가되기는 했지만, 앞서 소개한 델타 규칙의 학습 절차와 크게 다르지 않습니다.
