---
layout: entry
post-category: ai
title: Deep Learning for Beginners - 딥러닝
author: 김성중
author-email: ajax0615@gmail.com
description: Deep Learning of Beginners의 딥러닝 챕터에 대한 정리입니다.
publish: true
---

## **1. 개요**
딥러닝을 간단하게 정의하면 '심층 신경망을 이용한 머신러닝 기법'이라고 할 수 있습니다. 심층 신경망은 은닉층이 2개 이상인 다층 신경망을 의미합니다.

![deep-learning](/images/2017/04/26/deep-learning.png "deep-learning"){: .center-image }

딥러닝의 역사에 대해 간단히 이야기하자면, 단층 신경망의 한계를 극복한 다층 신경망이 나오기까지 30여 년이 걸렸는데, 그 이유는 학습 규칙을 찾지 못했기 때문입니다. 이 문제는 **역전파 알고리즘** 의 개발로 해결되었습니다. 반면 심층 신경망 기반의 딥러닝이 등장하기까지 다시 20여 년이 걸린 까닭은 성능 문제 때문이었습니다. 다층 신경망의 은닉층을 늘려 역전파 알고리즘으로 학습시키면 오히려 성능이 더 떨어지는 경우가 많았습니다. 딥러닝은 이 문제를 해결했습니다.

---

## **2. 심층 신경망의 개선**
탁월한 성과에도 불구하고 딥러닝에는 결정적이라고 할 만한 기술은 존재하지 않습니다. 딥러닝의 혁신은 여러 가지 작은 기술적 개선들이 모인 결과입니다.

신경망의 계층을 깊게 만들면 오히려 성능이 더 떨어지는 이유는 신경망이 제대로 학습되지 않기 때문입니다. 역전파 알고리즘으로 심층 신경망을 학습시킬 때 겪는 어려움은 크게 세 가지입니다.

- 그래디언트 소실(vanishing gradient)
- 과적합
- 많은 계산량

#### 그래디언트 소실
그래디언트 소실은 역전파 알고리즘으로 심층 신경망을 학습시키는 과정에서, **출력층에서 멀어질수록 신경망의 출력 오차가 반영되지 않는 현상** 을 말합니다. 역전파 알고리즘은 출력층의 오차를 은닉층으로 역전파시켜 신경망을 학습시킵니다. 그런데 앞쪽의 은닉층까지는 오차가 거의 전달되지 않으니 가중치도 변하지 않게 되어 입력층에 가까운 은닉층들은 제대로 학습되지 않는 겁니다. 학습이 제대로 되지 않으면 은닉층을 추가한 의미가 없어집니다.

![hidden-layer](/images/2017/04/26/hidden-layer.png "hidden-layer"){: .center-image }

그래디언트 소실에 대한 대표적인 대처 방안은 노드들의 활성함수를 ReLU *Rectified Linear Unit* 함수로 바꾸는 것입니다. 기존의 시그모이드 함수보다는 오차가 훨씬 더 잘 전달된다고 합니다. ReLU 함수는 입력이 음수이면 0을 출력하고, 양수이면 입력값 그대로 출력하는 함수입니다.

![ReLU](/images/2017/04/26/ReLU.png "ReLU"){: .center-image }

시그모이드 함수의 경우에는 입력값이 아무리 커도 신경망 노드의 출력은 1을 넘지 못했습니다. ReLU 함수는 이런 제한이 없습니다.

이 밖에도 Cross Entropy 함수로부터 유도된 학습 규칙을 사용하면 신경망의 학습 성능을 높일 수 있습니다.

#### 과적합
두 번째로 심층 신경망의 과적합 문제입니다. 심층 신경망이 과적합 문제에 더 취약한 이유는 **은닉층이 늘어나면 연결 가중치도 많아져 더 복잡한 모델이 되기 때문** 입니다. 신경망의 성능을 높이려면 은닉층을 늘려 계층 구조를 더 깊게 해야 하는데, 머신러닝의 난제인 과적합 문제에는 더 취약해지는 딜레마에 빠지게 됩니다.

가장 대표적인 해결책은 드롭아웃 *Dropout* 이라는 기법입니다. 드롭아웃은 신경망 전체를 다 학습시키지 않고 일부 노드만 무작위로 골라 학습시키는 기법입니다. 구현은 별로 복잡하지 않으면서도 효과는 매우 뛰어나다고 합니다. 드롭아웃은 학습하는 중간중간 일정 비율로 노드들을 무작위로 골라 출력을 0으로 만들어 신경망의 출력을 계산합니다.

![dropout](/images/2017/04/26/dropout.png "dropout"){: .center-image }

드롭아웃을 적용하면 학습되는 노드와 가중치들이 매번 달라져, 신경망이 과적합에 빠지는 걸 효과적으로 예방할 수 있다고 합니다. 드롭아웃하는 비율은 은닉층의 경우에는 50% 정도이고, 입력 노드에 적용하는 경우에는 26% 정도가 일반적입니다.

아울러 비용함수에 가중치의 크기에 해당되는 정칙화 항을 추가해 학습시키는 방법도 과적합 개선을 위해 많이 사용됩니다. 신경망의 구조를 최대한 단순하게 만들어 과적합 될 여지를 줄이는 원리입니다. 또한 대량의 학습 데이터 사용도 과적합 방지에 큰 도움이 됩니다. 학습 데이터가 많으면 그만큼 신경망이 특정 데이터에 의해 편향될 여지가 줄어들기 때문입니다.

#### 많은 계산량
마지막 난제는 학습 시간이 너무 오래 걸리는 문제입니다. 은닉층이 많을수록 연결 가중치가 기하급수적으로 늘어나고 학습 데이터도 많이 필요해 계산량이 급증합니다. 계산량이 많으면 결국 학습하는 시간도 더 오래 걸립니다. 이 문제는 실제로 신경망의 개발에 상당히 걸림돌이 됩니다. 만약 어떤 심층 신경망의 학습에 한 달이 걸린다면, 1년에 12번밖에 신경망을 수정할 수 없게 됩니다. 이런 환경에서는 제대로 된 연구를 하기가 거의 불가능합니다. 학습 소요 시간 문제는 GPU 같은 고성능 하드웨어의 등장과 배치 정규화 *batch normalization* 등 여러 알고리즘 덕분에 상당 부분 개선되었습니다.

---

## **Reference**
[Deep Learning for Beginners](https://deeplearning4j.org/deeplearningforbeginners.html)
